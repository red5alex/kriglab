{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORDINARY KRIGING - 1D - Single Point\n",
    "\n",
    "**Acknowledgements: ** This Notebook is based on an original blogpost by Connor Johnson, \n",
    "http://connor-johnson.com/2014/03/20/simple-kriging-in-python/.\n",
    "\n",
    "In that blogpost, the data set used (ZoneA.dat) was available from Geoffrey C. Bohling's webpage hosted by the University of Kansas. It does not seem to be available from there any longer. The version of the data that is provided in this repository (kriglab) was downloaded from a link from a notebook (https://colab.research.google.com/github/agile-geoscience/xlines/blob/master/notebooks/11_Gridding_map_data.ipynb#scrollTo=tKrxguz-vpI_) by Matt Hall (https://www.linkedin.com/in/geoscientist/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Post\n",
    "\n",
    "In this post I will work through an example of Simple Kriging. Kriging is a set of techniques for interpolation. It differs from other interpolation techniques in that it sacrifices smoothness for the integrity of sampled points. Most interpolation techniques will over or undershoot the value of the function at sampled locations, but kriging honors those measurements and keeps them fixed. In future posts I would like to cover other types of kriging, other semivariaogram models, and colocated co-kriging. Until then, I’m keeping relatively up to date code at my GitHub project, geostatsmodels. (https://github.com/cjohnson318/geostatsmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['figure.figsize'] = [16 / 1.5, 10 / 1.5]   # inch / cm = 2.54\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "# plt.rcParams['savefig.frameon'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "The data used in this exercise is in a zip file at this site. (Click on Geostatistics Resources.) I have adapted my material from the Kriging document on the same site. My approach will focus more on programming. First we will import some modules and then load the data and parse it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zraw = open( 'example_data/ZoneA.dat','r' ).readlines()\n",
    "zraw = [ i.strip().split() for i in zraw[10:] ]\n",
    "zraw = np.array( zraw, dtype=np.float )\n",
    "zraw = DataFrame( zraw, columns=['x','y','thk','por','perm','lperm','lpermp','lpermr'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a sub-set and reduce to Quasi-1D\n",
    "\n",
    "For this exercise, we need to reduce the data, and only choose values within a certain intervall for y, and create a quasi-1D Dataset by zeroing the y-coordinate. This is necessary as the method may fail during matrix inversion if there are two points with (near) equal x-coordinates, leading to a singular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the data subset from a y-intervall\n",
    "ymean = 7500\n",
    "yinterval = 2000\n",
    "\n",
    "z = zraw  # create copy of full data set\n",
    "\n",
    "# show the intervall before reducing the data\n",
    "fig, ax = subplots()\n",
    "ax.scatter( z.x, z.y, c=z.por, cmap='gray' )\n",
    "ax.plot((-1500,22000),(ymean - yinterval / 2, ymean - yinterval / 2), \"r--\")\n",
    "ax.plot((-1500,22000),(ymean + yinterval / 2, ymean + yinterval / 2), \"r--\")\n",
    "ax.plot((-1500,22000),(ymean, ymean), \"r-\")\n",
    "ax.set_aspect(1)\n",
    "xlim(-1500,22000)\n",
    "ylim(-1500,17500)\n",
    "xlabel('Easting [m]')\n",
    "ylabel('Northing [m]')\n",
    "title('Porosity %') ;\n",
    "\n",
    "plt.show()\n",
    "\n",
    "z = z.loc[z.y > ymean - yinterval / 2]\n",
    "z = z.loc[z.y < ymean + yinterval / 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce to quasi 1-D by assigning zero as y-coordinates for all samples,\n",
    "# and sort the array by x-coordinate for better plottability.\n",
    "\n",
    "z.y = 0.\n",
    "z = z.sort_values(by=\"x\")\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.scatter( z.x, z.y, c=z.por, cmap='gray' )\n",
    "ylabel('Northing [m]')\n",
    "title('Porosity %') ;\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(z.x, z.por, \".--\")\n",
    "ylabel('Value')\n",
    "xlabel('Easting [m]')\n",
    "plt.show()\n",
    "\n",
    "# part of our data set recording porosity\n",
    "P = np.array( z[['x','y','por']] )\n",
    "print(\"P =\")\n",
    "print(P)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Semivariogram\n",
    "\n",
    "The semivariogram encodes data about spatial variance over the region at a given distance or lag. We generally expect data points that are close together spatially to share other characteristics, and we expect points that are separated by greater distances to have lesser correlation. The semivariogram allows us to model the similarity points in a field as a function of distance. The semivariogram is given by,\n",
    "\n",
    "$$\\begin{equation*} \\hat{\\gamma}(h) = \\dfrac{1}{2N(h)} \\displaystyle \\sum_{N(h)} ( z_{i} - z_{j} )^{2} \\end{equation*}$$ (Equation 1)\n",
    "\n",
    "Here, h is distance specified by the user, and z_{i} and z_{j} are two points that are separated spatially by h. The N(h) term is the number of points we have that are separated by the distance h. The semivariogram then is the sum of squared differences between values separated by a distance h. As an aside, contrast this with the formulation for variance,\n",
    "\n",
    "$$\\begin{equation*} s = \\dfrac{1}{N-1} \\displaystyle \\sum_{k=1}^{N} (z_{k} - \\hat{\\mu} )^{2} \\end{equation*}$$ (Equation 2)\n",
    "\n",
    "Here, $N$ is the number of data points, $\\hat{\\mu}$ is the sample mean, and $z_{k}$ is a data point. For sample variance, we are taking the squared difference between data points and the mean, and in the semivariogram we are taking the squared difference between data points separated by distance h. We can write some functions to calculate the semivariogram at one lag, and then at multiple lags as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVh( P, h, bw ):\n",
    "    '''\n",
    "    Experimental semivariogram for a single lag\n",
    "    '''\n",
    "    pd = squareform( pdist( P[:,:2] ) )\n",
    "    N = pd.shape[0]\n",
    "    Z = list()\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N):\n",
    "            if( pd[i,j] >= h-bw )and( pd[i,j] <= h+bw ):\n",
    "                Z.append( ( P[i,2] - P[j,2] )**2.0 )\n",
    "    return np.sum( Z ) / ( 2.0 * len( Z ) )\n",
    " \n",
    "def SV( P, hs, bw ):\n",
    "    '''\n",
    "    Experimental variogram for a collection of lags\n",
    "    '''\n",
    "    sv = list()\n",
    "    for h in hs:\n",
    "        sv.append( SVh( P, h, bw ) )\n",
    "    sv = [ [ hs[i], sv[i] ] for i in range( len( hs ) ) if sv[i] > 0 ]\n",
    "    return np.array( sv ).T\n",
    " \n",
    "def C( P, h, bw ):\n",
    "    '''\n",
    "    Calculate the sill\n",
    "    '''\n",
    "    c0 = np.var( P[:,2] )\n",
    "    if h == 0:\n",
    "        return c0\n",
    "    return c0 - SVh( P, h, bw )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The C() function is the covariance function, and will be used later. Let us now calculate and plot the semivariogram,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bandwidth, plus or minus 250 meters\n",
    "bw = 500\n",
    "# lags in 500 meter increments from zero to 10,000\n",
    "hs = np.arange(0,10500,bw)\n",
    "sv = SV( P, hs, bw )\n",
    "plot( sv[0], sv[1], '.-' )\n",
    "xlabel('Lag [m]')\n",
    "ylabel('Semivariance')\n",
    "title('Empirical Semivariogram') ;\n",
    "savefig('sample_semivariogram.png', fmt='png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling\n",
    "Now that we’ve calculated the semivariogram, we will need to fit a model to the data. There are three popular models, the spherical, exponential, and the Gaussian. Here, we’ll implement the spherical model. First, we will present a function named opt() for determining the optimal value a for the spherical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt( fct, x, y, C0, parameterRange=None, meshSize=1000 ):\n",
    "    if parameterRange == None:\n",
    "        parameterRange = [ x[1], x[-1] ]\n",
    "    mse = np.zeros( meshSize )\n",
    "    a = np.linspace( parameterRange[0], parameterRange[1], meshSize )\n",
    "    for i in range( meshSize ):\n",
    "        mse[i] = np.mean( ( y - fct( x, a[i], C0 ) )**2.0 )\n",
    "    return a[ mse.argmin() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The opt() function finds the optimal parameter for fitting a spherical model to the semivariogram data. The spherical model is given by the function spherical(). On the last line we see that spherical() returns itself in a map() function, which seems odd. The idea is that the input h can be a single float value, or list or NumPy array of floats. If h is a single value, then line 9 is called. If h is a list or an array (an iterable) then line 17 is called, which applies line 9 to each value of h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spherical( h, a, C0 ):\n",
    "    '''\n",
    "    Spherical model of the semivariogram\n",
    "    '''\n",
    "    # if h is a single digit\n",
    "    if type(h) == np.float64:\n",
    "        # calculate the spherical function\n",
    "        if h <= a:\n",
    "            return C0*( 1.5*h/a - 0.5*(h/a)**3.0 )\n",
    "        else:\n",
    "            return C0\n",
    "    # if h is an iterable\n",
    "    else:\n",
    "        # calcualte the spherical function for all elements\n",
    "        a = np.ones( h.size ) * a\n",
    "        C0 = np.ones( h.size ) * C0\n",
    "        return list(map( spherical, h, a, C0 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, cvmodel() fits a model to the semivariogram data and returns a covariance method named covfct()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvmodel( P, model, hs, bw ):\n",
    "    '''\n",
    "    Input:  (P)      ndarray, data\n",
    "            (model)  modeling function\n",
    "                      - spherical\n",
    "                      - exponential\n",
    "                      - gaussian\n",
    "            (hs)     distances\n",
    "            (bw)     bandwidth\n",
    "    Output: (covfct) function modeling the covariance\n",
    "    '''\n",
    "    # calculate the semivariogram\n",
    "    sv = SV( P, hs, bw )\n",
    "    # calculate the sill\n",
    "    C0 = C( P, hs[0], bw )\n",
    "    # calculate the optimal parameters\n",
    "    param = opt( model, sv[0], sv[1], C0 )\n",
    "    # return a covariance function\n",
    "    covfct = lambda h, a=param: C0 - model( h, a, C0 )\n",
    "    return covfct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we’ll plot our model and see if it represents our data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sp = cvmodel( P, model=spherical, hs=np.arange(0,10500,500), bw=500 )\n",
    "C0 = C( P, hs[0], bw )\n",
    "plot( sv[0], sv[1], '.-' )\n",
    "plot( sv[0], C0 - sp( sv[0] ) ) ;\n",
    "title('Spherical Model')\n",
    "ylabel('Semivariance')\n",
    "xlabel('Lag [m]')\n",
    "savefig('semivariogram_model.png',fmt='png',dpi=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Kriging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*from simple kriging:*\n",
    "\n",
    "Now that we have a model for the semivariogram, we can write a function to perform the kriging. The fundamental relationship is a matrix equation,\n",
    "\n",
    "$$\\begin{equation*} K w = k \\Rightarrow w = K^{-1} k \\end{equation*} $$ (Equation 1)\n",
    "\n",
    "Here, $K$ is a matrix of covariances of sampled data points, calculated using the spherical model, $\\lambda$ is a vector of simple kriging weights, and $k$ is the vector of covariances between the data points and an unsampled point. Our kriging function takes the data set $P$, the model, the distances hs, the bandwidth bw, the coordinates of the unsampled point u, and the number of surrounding points N to use in the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of ordinary kriging (OK) also a constant mean is assumed, but this time it is assumed to be unknown and only constant for local nearby estimation points. Due to that the sum of the kriging weights w has to be one.\n",
    "\n",
    "$$ \\hat{Z}_{OK}(u) = \\Sigma^N_{i=1} w_i Z(u_i)$$ with $$  \\Sigma^N_{i=1} w_i = 1$$\n",
    "\n",
    "As already explained in 3.2 the aim with kriging is to minimize the estimation variance. This aim to-gether with the unbiasedness constrain that the weights have to sum up to one, leads to the need of a Lagrange multiplier $\\lambda$ when using ordinary kriging. The Lagrangian multiplier method helps to solve an optimization problem (in our case the minimization of the estimator variance) with auxilia-ry conditions (weights have to sum up to one). Written in matrix and vector form, the ordinary kriging equation can then be formulated as:\n",
    "\n",
    "$$ \\boldsymbol{K} = \n",
    "\\begin{bmatrix} \\boldsymbol{K_1} & \\boldsymbol{1}\\\\ 1 & 0 \\end{bmatrix}\n",
    "\\begin{bmatrix} \\boldsymbol{w_1}\\\\ \\lambda \\end{bmatrix}=\n",
    "\\begin{bmatrix} \\boldsymbol{k_0}\\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "with\n",
    "\n",
    "$$ K_1 = \\begin{bmatrix} C_{11} & \\dots & C_{n1}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "C_{1n} & \\dots & C_{nn} \\end{bmatrix} $$\n",
    "\n",
    "$$ w_1 = \\begin{bmatrix} w_1\\\\\n",
    "\\vdots \\\\\n",
    "w_n\\end{bmatrix} $$\n",
    "\n",
    "$$ k_0 = \\begin{bmatrix} c_{10}\\\\\n",
    "\\vdots \\\\\n",
    "c_{n0}\\end{bmatrix} $$\n",
    "\n",
    "The first term is the covariance matrix K of equation 1, the second the weight vector w and on the right hand side of the equal sign the covariance vector k.\n",
    "To solve the equation for the weights and lambda:\n",
    "\n",
    "$$ \\begin{bmatrix} \\boldsymbol{w_1}\\\\ \\lambda \\end{bmatrix} =\n",
    "\\begin{bmatrix} \\boldsymbol{K_1} & \\boldsymbol{1}\\\\ 1 & 0 \\end{bmatrix}^{-1}\n",
    "\\begin{bmatrix} \\boldsymbol{k_0}\\\\ 1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h, k, K, weights = krige( P, sp, (dy*j,dx*i), len(z) / 2 )       \n",
    "# PARAMETER DEFINITION\n",
    "#           (P)     data set [y,x,value]\n",
    "#           (model) modeling function\n",
    "#                    - spherical\n",
    "#                    - exponential\n",
    "#                    - gaussian\n",
    "#           (u)     unsampled point\n",
    "#           (N)     number of neighboring\n",
    "#                   points to consider\n",
    "\n",
    "# P - Data set already defined\n",
    "covfct = sp  # covariance function\n",
    "u = (12000, 0)  # (x,y) - coordinates of unsampled points\n",
    "N = 5  # number of neighbors to consider\n",
    "assert N < len(P) + 1, \"Number of Neighbors greater than number of data points\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Kriging Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Vector (Unsampled Point <-> Sampled Points)\n",
    "\n",
    "The distance between the unsampled point $u$ and each data point in $P$ is calculated, creating a vector $d$ of size 1xM, where M is the number of data points.\n",
    "\n",
    "This vector becomes an additional column to the matrix $P$, which allows to sort the rows of $P$ by their distance to the data point, and only selecting to closest N neighbors.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance between u and each data point in P (1xN vector)\n",
    "d = np.sqrt( ( P[:,0]-u[0] )**2.0 + ( P[:,1]-u[1] )**2.0 )  # 2D \n",
    "print(\"d = \"+str(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add these distances to P\n",
    "P_n = np.vstack(( P.T, d )).T\n",
    "# sort P by these distances\n",
    "# take the first N of them\n",
    "P_n = P_n[d.argsort()[:N]]\n",
    "print(\"P_n = \\n\" + str(P_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance Vector $\\boldsymbol{k_0}$\n",
    "\n",
    "For each of the remaining points, the expected covariance between the point and the unsampled point $u$ is calculated based on its distance by the convariance function\n",
    "\n",
    "$$\\boldsymbol{k_0} = cov(\\boldsymbol{d})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the covariance model to the distances\n",
    "k = covfct( P_n[:,3] )\n",
    "# cast as a matrix\n",
    "k = np.matrix( k ).T\n",
    "print(\"k =\\n\" + str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance Matrix $\\boldsymbol{K_1}$\n",
    "\n",
    "To create the covariance matrix $K_1$ with the covariances of all sampled points, the same function is being used:\n",
    "\n",
    "$$K_1 = [cov(d_{ij})]$$\n",
    "\n",
    "with $d_{ij}$ being the Euclidian distance between points $i$ and $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form a matrix of distances between existing data points\n",
    "K_1 = squareform( pdist( P_n[:,:2] ) )\n",
    "# apply the covariance model to these distances\n",
    "K_1 = covfct( K_1.ravel() )\n",
    "# re-cast as a NumPy array -- thanks M.L.\n",
    "K = np.array( K_1 )\n",
    "# reshape into an array\n",
    "K_1 = K_1.reshape(N,N)\n",
    "# cast as a matrix\n",
    "K_1 = np.matrix( K_1 )\n",
    "print(\"K_1 =\\n\"+str(K_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducting the LaGrange Factor\n",
    "\n",
    "Ordinary Kriging requires the Matrix equation system to be extended\n",
    "\n",
    "$$ \\boldsymbol{K} = \n",
    "\\begin{bmatrix} \\boldsymbol{K_1} & \\boldsymbol{1}\\\\ 1 & 0 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing the LaGrange Factor\n",
    "K_line1 = np.hstack([K_1 , np.ones((K_1.shape[0],1))])\n",
    "K_line2 = np.hstack([np.ones((1, K_1.shape[0])),np.zeros((1,1))])\n",
    "K = np.vstack([K_line1, K_line2])\n",
    "print(\"K =\\n\"+str(K_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$k = \\begin{bmatrix} \\boldsymbol{k_0}\\\\ 1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = np.vstack([k,1])\n",
    "print(\"k =\\n\" + str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving for the weights\n",
    "\n",
    "this allows calculating the Kriging weights $w$ and LaGrange factor $\\lambda$ as defined previously\n",
    "\n",
    "$$ \\begin{bmatrix} \\boldsymbol{w_1}\\\\ \\lambda \\end{bmatrix} =\n",
    "\\begin{bmatrix} \\boldsymbol{K_1} & \\boldsymbol{1}\\\\ \\boldsymbol{1} & 0 \\end{bmatrix}^{-1}\n",
    "\\begin{bmatrix} \\boldsymbol{k_0}\\\\ 1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the kriging weights\n",
    "lhs = np.linalg.inv( K ) * k\n",
    "lhs = np.array( lhs )\n",
    "\n",
    "weights = lhs[:-1]\n",
    "lagrange = weights[-1]\n",
    "\n",
    "print(\"weights =\\n\"+str(weights))\n",
    "print(\"laGrange =\\n\"+str(lagrange))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Prediction and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary Kriging:\n",
    "\n",
    "$$o = P[:,2]$$\n",
    "\n",
    "$$e(u) =w^T \\cdot o $$\n",
    "\n",
    "--- muss nochmal geprüft werden, vermutlich muss der LaGrange faktor noch addiert werde ---\n",
    "\n",
    "with $o$ being the vector of observed values at the sampling points, and $e(u)$ the estimate at the unsampled location $u$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = np.array(P_n[:,2])\n",
    "print(\"observations = \"+str(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the estimation\n",
    "estimation = np.dot( weights.T, o )\n",
    "print(\"estimation = \"+str(estimation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma^2 = C_0 - w^T \\cdot k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = C0 - np.dot( weights.T, k[:-1] )\n",
    "print(\"variance = \"+str(variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "N": "5"
    }
   },
   "source": [
    "## Visualize the Result\n",
    "\n",
    "The plot below shows the estimate (error bar equals +/- 1 standard deviation) and the sampled points. For the {{N}} samples closest to the estimate the kriging weights are shown.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(z.x, z.por, \n",
    "         \"o--\", label=\"samples\")\n",
    "plt.scatter((u[0]), (estimation), \n",
    "            c=\"red\", label=\"estimate\")\n",
    "plt.errorbar((u[0]), (estimation), yerr = sqrt(float(variance)), \n",
    "             fmt=\"r\", label=\"standard deviation\")\n",
    "\n",
    "offset = 0.05\n",
    "for i in range(len(weights)):\n",
    "    plt.text(P_n[i,0]+offset, \n",
    "             P_n[i,2]+offset, \n",
    "             \"{:.2f}\".format(float(weights[i])))\n",
    "\n",
    "ylabel('Value')\n",
    "xlabel('Easting [m]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
